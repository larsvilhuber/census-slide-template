% This file was created with JabRef 2.10.
% Encoding: ISO8859_1


@InCollection{AbowdEtAl2009,
  Title                    = {The {LEHD} Infrastructure Files and the Creation of the {Q}uarterly {W}orkforce {I}ndicators},
  Author                   = {John M. Abowd and Bryce E. Stephens and Lars Vilhuber and Fredrik Andersson and Kevin L. McKinney and Marc Roemer and Simon D. Woodcock},
  Booktitle                = {Timothy Dunne, J. Bradford Jensen, and Mark J. Roberts},
  Publisher                = {University of Chicago Press},
  Year                     = {2009},

  Crossref                 = {DunneJensenRoberts2009},
  File                     = {AbowdStephensVilhuber2005-LEHD-final.pdf:L/LEHD/AbowdStephensVilhuber2005-LEHD-final.pdf:PDF},
  Owner                    = {vilhuber},
  Timestamp                = {2007.03.13}
}

@TechReport{AbowdEtAl2012,
  Title                    = {Dynamically consistent noise infusion and partially synthetic data as confidentiality protection measures for related time-series},
  Author                   = {John M. Abowd and Kaj Gittings and Kevin L. McKinney and Bryce E. Stephens and Lars Vilhuber and Simon Woodcock},
  Institution              = {Federal Committee on Statistical Methodology},
  Year                     = {2012},
  Month                    = {January},

  Owner                    = {vilhuber},
  Timestamp                = {2012.05.21},
  Url                      = {http://www.fcsm.gov/events/papers2012.html}
}

@Article{RePEc:taf:japsta:v:39:y:2012:i:2:p:243-265,
  Title                    = {New data dissemination approaches in old {E}urope -- synthetic datasets for a {G}erman establishment survey},
  Author                   = {J\"org Drechsler},
  Journal                  = {Journal of Applied Statistics},
  Year                     = {2012},

  Month                    = {April},
  Number                   = {2},
  Pages                    = {243-265},
  Volume                   = {39},

  Abstract                 = { Disseminating microdata to the public that provide a high level of data utility, while at the same time guaranteeing the confidentiality of the survey respondent is a difficult task. Generating multiply imputed synthetic datasets is an innovative statistical disclosure limitation technique with the potential of enabling the data disseminating agency to achieve this twofold goal. So far, the approach was successfully implemented only for a limited number of datasets in the U.S. In this paper, we present the first successful implementation outside the U.S.: the generation of partially synthetic datasets for an establishment panel survey at the German Institute for Employment Research. We describe the whole evolution of the project: from the early discussions concerning variables at risk to the final synthesis. We also present our disclosure risk evaluations and provide some first results on the data utility of the generated datasets. A variance-inflated imputation model is introduced that incorporates additional variability in the model for records that are not sufficiently protected by the standard synthesis.},
  Url                      = {http://ideas.repec.org/a/taf/japsta/v39y2012i2p243-265.html}
}

@TechReport{RePEc:iab:iabfme:201101_de,
  Title                    = {Synthetische {S}cientific-Use-Files der {W}elle 2007 des {IAB}-{B}etriebspanels},
  Author                   = {Drechsler, J\"org},
  Institution              = {Institute for Employment Research, Nuremberg, Germany},
  Year                     = {2011},
  Month                    = Jan,
  Number                   = {201101\_de},
  Type                     = {FDZ Methodenreport},

  Abstract                 = {\&quot;Providing scientific use files for business surveys is a difficult task. Due to smaller populations, higher sampling rates, and skewed distributions disclosure risks are much higher than for household surveys. Simple measures like coarsening are not sufficient to protect the data. The aim of generating synthetic datasets is to release data that provide a high level of data utility while guaranteeing the confidentiality of the survey respondent. To achieve this, sensitive variables and variables that could be used for re-identification purposes are replaced with multiple imputations. This report gives a short introduction to the topic and discusses some aspects that analysts should keep in mind when using the synthetic datasets. Furthermore, the report describes how valid inferences can be obtained based on the synthetic datasets and provides some first data utility evaluations that indicate the potentials but also the limits of the generated datasets.\&quot; (Author's abstract, IAB-Doku) ((en))},
  Keywords                 = {IAB-Betriebspanel; Imputationsverfahren; Datenanonymisierung; Datenschutz; Betriebsdatenerfassung; N},
  Url                      = {http://ideas.repec.org/p/iab/iabfme/201101_de.html}
}
}

@Article{RePEc:eee:csdana:v:55:y:2011:i:12:p:3232-3243,
  Title                    = {An empirical evaluation of easily implemented, nonparametric methods for generating synthetic datasets},
  Author                   = {Drechsler, J\"org and Reiter, Jerome P.},
  Journal                  = {Computational Statistics \& Data Analysis},
  Year                     = {2011},

  Month                    = {December},
  Number                   = {12},
  Pages                    = {3232-3243},
  Volume                   = {55},

  Abstract                 = {When intense redaction is needed to protect the confidentiality of data subjects' identities and sensitive attributes, statistical agencies can use synthetic data approaches. To create synthetic data, the agency replaces identifying or sensitive values with draws from statistical models estimated from the confidential data. Many agencies are reluctant to implement this idea because (i) the quality of the generated data depends strongly on the quality of the underlying models, and (ii) developing effective synthesis models can be a labor-intensive and difficult task. Recently, there have been suggestions that agencies use nonparametric methods from the machine learning literature to generate synthetic data. These methods can estimate non-linear relationships that might otherwise be missed and can be run with minimal tuning, thus considerably reducing burdens on the agency. Four synthesizers based on machine learning algorithms-classification and regression trees, bagging, random forests, and support vector machines-are evaluated in terms of their potential to preserve analytical validity while reducing disclosure risks. The evaluation is based on a repeated sampling simulation with a subset of the 2002 Uganda census public use sample data. The simulation suggests that synthesizers based on regression trees can result in synthetic datasets that provide reliable estimates and low disclosure risks, and that these synthesizers can be implemented easily by statistical agencies.},
  Keywords                 = { Census Confidentiality Disclosure Imputation Microdata Synthetic},
  Url                      = {http://ideas.repec.org/a/eee/csdana/v55y2011i12p3232-3243.html}
}

@Article{DrechslerReiter2009,
  Title                    = {Disclosure Risk and Data Utility for Partially Synthetic Data: An Empirical Study Using the {G}erman {IAB} {E}stablishment {S}urvey. , Vol. 25, 589-603.},
  Author                   = {Drechsler, J\"org and Reiter, Jerome P.},
  Journal                  = {Journal of Official Statistics},
  Year                     = {2009},

  Month                    = {December},
  Number                   = {12},
  Pages                    = {589-603},
  Volume                   = {25},

  Abstract                 = {When intense redaction is needed to protect the confidentiality of data subjects' identities and sensitive attributes, statistical agencies can use synthetic data approaches. To create synthetic data, the agency replaces identifying or sensitive values with draws from statistical models estimated from the confidential data. Many agencies are reluctant to implement this idea because (i) the quality of the generated data depends strongly on the quality of the underlying models, and (ii) developing effective synthesis models can be a labor-intensive and difficult task. Recently, there have been suggestions that agencies use nonparametric methods from the machine learning literature to generate synthetic data. These methods can estimate non-linear relationships that might otherwise be missed and can be run with minimal tuning, thus considerably reducing burdens on the agency. Four synthesizers based on machine learning algorithms-classification and regression trees, bagging, random forests, and support vector machines-are evaluated in terms of their potential to preserve analytical validity while reducing disclosure risks. The evaluation is based on a repeated sampling simulation with a subset of the 2002 Uganda census public use sample data. The simulation suggests that synthesizers based on regression trees can result in synthetic datasets that provide reliable estimates and low disclosure risks, and that these synthesizers can be implemented easily by statistical agencies.},
  Keywords                 = { Census Confidentiality Disclosure Imputation Microdata Synthetic},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12},
  Url                      = {http://ideas.repec.org/a/eee/csdana/v55y2011i12p3232-3243.html}
}

@TechReport{DrechslerVilhuber2013,
  Title                    = {Replicating the {S}ynthetic {LBD} with {G}erman Establishment Data},
  Author                   = {Drechsler, J\"org and Vilhuber, Lars},
  Institution              = {World Statistics Conference},
  Year                     = {2013},
  Type                     = {Presentation},

  Owner                    = {vilhuber},
  Timestamp                = {2013.06.16}
}


@Article{KinneyEtAl2011,
  Title                    = {Towards Unrestricted Public Use Business Microdata: The {S}ynthetic {L}ongitudinal {B}usiness {D}atabase},
  Author                   = {Satkartar K. Kinney and Jerome P. Reiter and Arnold P. Reznek and Javier Miranda and Ron S. Jarmin and John M. Abowd},
  Journal                  = {International Statistical Review},
  Year                     = {2011},

  Month                    = {December},
  Number                   = {3},
  Pages                    = {362-384},
  Volume                   = {79},

  Abstract                 = {In most countries, national statistical agencies do not release establishment-level business microdata, because doing so represents too large a risk to establishments\' confidentiality. One approach with the potential for overcoming these risks is to release synthetic data; that is, the released establishment data are simulated from statistical models designed to mimic the distributions of the underlying real microdata. In this article, we describe an application of this strategy to create a public use file for the Longitudinal Business Database, an annual economic census of establishments in the United States comprising more than 20 million records dating back to 1976. The U.S. Bureau of the Census and the Internal Revenue Service recently approved the release of these synthetic microdata for public use, making the synthetic Longitudinal Business Database the first-ever business microdata set publicly released in the United States. We describe how we created the synthetic data, evaluated analytical validity, and assessed disclosure risk.},
  File                     = {KinneyEtAl2011.pdf:K/KinneyEtAl2011.pdf:PDF},
  Owner                    = {vilhuber},
  Timestamp                = {2012.05.18},
  Url                      = {http://ideas.repec.org/a/bla/istatr/v79y2011i3p362-384.html}
}

@Article{little93,
  Title                    = {Statistical Analysis of Masked Data},
  Author                   = {Roderick J.A. Little},
  Journal                  = {Journal of Official Statistics},
  Year                     = {1993},
  Number                   = {2},
  Pages                    = {407-426},
  Volume                   = {9},

  Owner                    = {John Abowd},
  Timestamp                = {2008.04.29}
}

@Article{Ashwin2008,
  Title                    = {Privacy: {T}heory meets practice on the map},
  Author                   = {Ashwin Machanavajjhala and Daniel Kifer and John M. Abowd and Johannes Gehrke and Lars Vilhuber},
  Journal                  = {International Conference on Data Engineering (ICDE)},
  Year                     = {2008},

  Owner                    = {vilhuber},
  Timestamp                = {2012.05.21}
}

@TechReport{MirandaVilhuber2013,
  Title                    = {Looking back on three years of {S}ynthetic {LBD} {B}eta},
  Author                   = {Miranda, Javier and Vilhuber, Lars},
  Institution              = {World Statistics Conference},
  Year                     = {2013},
  Type                     = {Presentation},

  Owner                    = {vilhuber},
  Timestamp                = {2013.06.16}
}

